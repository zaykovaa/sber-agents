import logging
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from config import config

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
vector_store = None
retriever = None
chunks = None  # –î–ª—è BM25 retriever
cross_encoder = None  # –î–ª—è reranking (lazy loading)

# –ö–µ—à–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ LLM –∫–ª–∏–µ–Ω—Ç–æ–≤
_conversational_answering_prompt = None
_retrieval_query_transform_prompt = None
_llm_query_transform = None
_llm = None

def create_semantic_retriever():
    """–°–æ–∑–¥–∞–Ω–∏–µ semantic retriever –∏–∑ vector store"""
    if vector_store is None:
        raise ValueError("Vector store not initialized")
    return vector_store.as_retriever(
        search_kwargs={'k': config.SEMANTIC_RETRIEVER_K}
    )

def create_bm25_retriever():
    """–°–æ–∑–¥–∞–Ω–∏–µ BM25 retriever –∏–∑ chunks"""
    if chunks is None or len(chunks) == 0:
        raise ValueError("Chunks not initialized for BM25")
    bm25 = BM25Retriever.from_documents(chunks)
    bm25.k = config.BM25_RETRIEVER_K
    return bm25

def create_hybrid_retriever():
    """–°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ retriever (Semantic + BM25)"""
    semantic = create_semantic_retriever()
    bm25 = create_bm25_retriever()
    
    logger.info(f"Hybrid retriever: semantic_k={config.SEMANTIC_RETRIEVER_K}, bm25_k={config.BM25_RETRIEVER_K}")
    logger.info(f"Ensemble weights: semantic={config.ENSEMBLE_SEMANTIC_WEIGHT}, bm25={config.ENSEMBLE_BM25_WEIGHT}")
    
    return EnsembleRetriever(
        retrievers=[semantic, bm25],
        weights=[config.ENSEMBLE_SEMANTIC_WEIGHT, config.ENSEMBLE_BM25_WEIGHT]
    )

def get_cross_encoder():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è cross-encoder –¥–ª—è reranking"""
    global cross_encoder
    if cross_encoder is None:
        try:
            from sentence_transformers import CrossEncoder
            logger.info(f"Loading cross-encoder model: {config.CROSS_ENCODER_MODEL}")
            cross_encoder = CrossEncoder(config.CROSS_ENCODER_MODEL)
            logger.info("‚úì Cross-encoder loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load cross-encoder: {e}", exc_info=True)
            raise
    return cross_encoder

def rerank_documents(query: str, documents: list, top_k: int = None):
    """
    –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é cross-encoder
    
    Args:
        query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        documents: –°–ø–∏—Å–æ–∫ Document –æ–±—ä–µ–∫—Ç–æ–≤
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ (default: config.RERANKER_TOP_K)
    
    Returns:
        List[tuple]: –°–ø–∏—Å–æ–∫ (document, score) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
    """
    if top_k is None:
        top_k = config.RERANKER_TOP_K
    
    if not documents:
        return []
    
    encoder = get_cross_encoder()
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—ã (query, document_text) –¥–ª—è cross-encoder
    pairs = [(query, doc.page_content) for doc in documents]
    
    # Cross-encoder –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
    scores = encoder.predict(pairs)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    
    logger.info(f"Reranked {len(documents)} documents, returning top {top_k}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º top_k –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö
    return ranked[:top_k]

def create_retriever():
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è retriever –ø–æ —Ä–µ–∂–∏–º—É"""
    mode = config.RETRIEVAL_MODE.lower()
    
    if mode == "semantic":
        logger.info("Creating semantic retriever")
        return create_semantic_retriever()
    
    elif mode == "hybrid":
        logger.info("Creating hybrid retriever (Semantic + BM25)")
        return create_hybrid_retriever()
    
    elif mode == "hybrid_reranker":
        logger.info("Creating hybrid retriever with reranker (Semantic + BM25 + Cross-encoder)")
        # –î–ª—è hybrid_reranker –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ hybrid retriever
        # Reranking –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω –≤ get_rag_chain()
        return create_hybrid_retriever()
    
    else:
        raise ValueError(f"Unknown retrieval mode: {mode}. Use 'semantic', 'hybrid', or 'hybrid_reranker'")

def initialize_retriever():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è retriever –ø–æ —Ä–µ–∂–∏–º—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞"""
    global retriever
    if vector_store is None:
        logger.error("Cannot initialize retriever: vector_store is None")
        return False
    
    try:
        retriever = create_retriever()
        logger.info(f"‚úì Retriever initialized in '{config.RETRIEVAL_MODE}' mode")
        return True
    except Exception as e:
        logger.error(f"Failed to initialize retriever: {e}", exc_info=True)
        return False

def format_chunks(chunks):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
    """
    if not chunks:
        return "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    
    formatted_parts = []
    for i, chunk in enumerate(chunks, 1):
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        source = chunk.metadata.get('source', 'Unknown')
        page = chunk.metadata.get('page', 'N/A')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏
        source_name = source.split('/')[-1] if '/' in source else source
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫
        formatted_parts.append(
            f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source_name}, —Å—Ç—Ä. {page}]\n{chunk.page_content}"
        )
    
    return "\n\n---\n\n".join(formatted_parts)

def format_sources(documents):
    """
    –ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ —Ñ–∞–π–ª–∞–º
    –§–æ—Ä–º–∞—Ç: "üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: file1.pdf (—Å—Ç—Ä. 3, 5), file2.pdf (—Å—Ç—Ä. 1)"
    """
    if not documents:
        return None
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —Ñ–∞–π–ª–∞–º
    sources_by_file = {}
    for doc in documents:
        source = doc.metadata.get('source', 'Unknown')
        source_name = source.split('/')[-1] if '/' in source else source
        page = doc.metadata.get('page', 'N/A')
        
        if source_name not in sources_by_file:
            sources_by_file[source_name] = []
        if page != 'N/A':
            sources_by_file[source_name].append(str(page))
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω–æ
    parts = []
    for filename, pages in sources_by_file.items():
        if pages:
            pages_str = ", ".join(sorted(set(pages), key=lambda x: int(x) if x.isdigit() else 0))
            parts.append(f"{filename} (—Å—Ç—Ä. {pages_str})")
        else:
            parts.append(filename)
    
    return "üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: " + ", ".join(parts)

def _load_prompts():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    global _conversational_answering_prompt, _retrieval_query_transform_prompt
    
    if _conversational_answering_prompt is not None:
        return _conversational_answering_prompt, _retrieval_query_transform_prompt
    
    try:
        conversation_system_text = config.load_prompt(config.CONVERSATION_SYSTEM_PROMPT_FILE)
        query_transform_text = config.load_prompt(config.QUERY_TRANSFORM_PROMPT_FILE)
        
        _conversational_answering_prompt = ChatPromptTemplate(
            [
                ("system", conversation_system_text),
                ("placeholder", "{messages}")
            ]
        )
        
        _retrieval_query_transform_prompt = ChatPromptTemplate.from_messages(
            [
                MessagesPlaceholder(variable_name="messages"),
                ("user", query_transform_text),
            ]
        )
        
        logger.info("Prompts loaded successfully")
        return _conversational_answering_prompt, _retrieval_query_transform_prompt
        
    except FileNotFoundError as e:
        logger.error(f"Prompt file not found: {e}")
        raise
    except Exception as e:
        logger.error(f"Error loading prompts: {e}", exc_info=True)
        raise

def _get_llm_query_transform():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –¥–ª—è query transformation —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _llm_query_transform
    if _llm_query_transform is None:
        _llm_query_transform = ChatOpenAI(
            model=config.MODEL_QUERY_TRANSFORM,
            temperature=0.4
        )
        logger.info(f"Query transform LLM initialized: {config.MODEL_QUERY_TRANSFORM}")
    return _llm_query_transform

def _get_llm():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π LLM —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _llm
    if _llm is None:
        _llm = ChatOpenAI(
            model=config.MODEL,
            temperature=0.9
        )
        logger.info(f"Main LLM initialized: {config.MODEL}")
    return _llm

def get_retrieval_query_transformation_chain():
    """–¶–µ–ø–æ—á–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞"""
    _, retrieval_query_transform_prompt = _load_prompts()
    return (
        retrieval_query_transform_prompt
        | _get_llm_query_transform()
        | StrOutputParser()
    )

def get_rag_chain():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è RAG-—Ü–µ–ø–æ—á–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è answer –∏ documents –≤ LCEL —Å—Ç–∏–ª–µ"""
    if retriever is None:
        raise ValueError("Retriever not initialized")
    
    conversational_answering_prompt, _ = _load_prompts()
    mode = config.RETRIEVAL_MODE.lower()
    
    # –î–ª—è hybrid_reranker —Ä–µ–∂–∏–º–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —à–∞–≥ reranking
    if mode == "hybrid_reranker":
        # LCEL —Ü–µ–ø–æ—á–∫–∞ —Å reranking: ensemble_docs ‚Üí rerank ‚Üí documents ‚Üí answer
        return (
            RunnablePassthrough.assign(
                ensemble_docs=get_retrieval_query_transformation_chain() | retriever
            )
            # –®–∞–≥ reranking: –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã cross-encoder
            | RunnablePassthrough.assign(
                documents=lambda x: [doc for doc, score in rerank_documents(
                    query=x["messages"][-1].content if x["messages"] else "",
                    documents=x["ensemble_docs"],
                    top_k=config.RERANKER_TOP_K
                )]
            )
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö documents
            | RunnablePassthrough.assign(
                answer=lambda x: (conversational_answering_prompt | _get_llm() | StrOutputParser()).invoke({
                    "context": format_chunks(x["documents"]),
                    "messages": x["messages"]
                })
            )
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ answer –∏ documents
            | (lambda x: {"answer": x["answer"], "documents": x["documents"]})
        )
    
    # –î–ª—è semantic –∏ hybrid —Ä–µ–∂–∏–º–æ–≤ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ –±–µ–∑ reranking
    # LCEL —Ü–µ–ø–æ—á–∫–∞ –≤ —Å—Ç–∏–ª–µ –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞
    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º documents —á–µ—Ä–µ–∑ query transformation
    return (
        RunnablePassthrough.assign(
            documents=get_retrieval_query_transformation_chain() | retriever
        )
        # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ documents
        | RunnablePassthrough.assign(
            answer=lambda x: (conversational_answering_prompt | _get_llm() | StrOutputParser()).invoke({
                "context": format_chunks(x["documents"]),
                "messages": x["messages"]
            })
        )
        # –®–∞–≥ 3: –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ answer –∏ documents
        | (lambda x: {"answer": x["answer"], "documents": x["documents"]})
    )

async def rag_answer(messages):
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç RAG —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
    
    Args:
        messages: —Å–ø–∏—Å–æ–∫ LangChain messages (HumanMessage, AIMessage)
    
    Returns:
        dict: {"answer": str, "documents": list[Document]}
    """
    if vector_store is None or retriever is None:
        logger.error("Vector store or retriever not initialized")
        raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
    
    rag_chain = get_rag_chain()
    result = await rag_chain.ainvoke({"messages": messages})
    return result

def get_vector_store_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    stats = {
        "status": "not initialized" if vector_store is None else "initialized",
        "count": 0,
        "retrieval_mode": config.RETRIEVAL_MODE,
        "embedding_provider": config.EMBEDDING_PROVIDER,
    }
    
    if vector_store is not None:
        doc_count = len(vector_store.store) if hasattr(vector_store, 'store') else 0
        stats["count"] = doc_count
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    if config.EMBEDDING_PROVIDER == "openai":
        stats["embedding_model"] = config.EMBEDDING_MODEL
    elif config.EMBEDDING_PROVIDER == "huggingface":
        stats["embedding_model"] = config.HUGGINGFACE_EMBEDDING_MODEL
        stats["device"] = config.HUGGINGFACE_DEVICE
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã retrieval —Ä–µ–∂–∏–º–∞
    if config.RETRIEVAL_MODE == "semantic":
        stats["semantic_k"] = config.SEMANTIC_RETRIEVER_K
    elif config.RETRIEVAL_MODE == "hybrid":
        stats["semantic_k"] = config.SEMANTIC_RETRIEVER_K
        stats["bm25_k"] = config.BM25_RETRIEVER_K
        stats["semantic_weight"] = config.ENSEMBLE_SEMANTIC_WEIGHT
        stats["bm25_weight"] = config.ENSEMBLE_BM25_WEIGHT
    elif config.RETRIEVAL_MODE == "hybrid_reranker":
        stats["semantic_k"] = config.SEMANTIC_RETRIEVER_K
        stats["bm25_k"] = config.BM25_RETRIEVER_K
        stats["semantic_weight"] = config.ENSEMBLE_SEMANTIC_WEIGHT
        stats["bm25_weight"] = config.ENSEMBLE_BM25_WEIGHT
        stats["cross_encoder_model"] = config.CROSS_ENCODER_MODEL
        stats["reranker_top_k"] = config.RERANKER_TOP_K
    
    return stats

