import logging
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from config import config

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
vector_store = None
retriever = None

# –ö–µ—à–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ LLM –∫–ª–∏–µ–Ω—Ç–æ–≤
_conversational_answering_prompt = None
_retrieval_query_transform_prompt = None
_llm_query_transform = None
_llm = None

def initialize_retriever():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è retriever –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    global retriever
    if vector_store is None:
        logger.error("Cannot initialize retriever: vector_store is None")
        return False
    
    retriever = vector_store.as_retriever(search_kwargs={'k': config.RETRIEVER_K})
    logger.info(f"Retriever initialized with k={config.RETRIEVER_K}")
    return True

def format_chunks(chunks):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
    """
    if not chunks:
        return "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    
    formatted_parts = []
    for i, chunk in enumerate(chunks, 1):
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        source = chunk.metadata.get('source', 'Unknown')
        page = chunk.metadata.get('page', 'N/A')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏
        source_name = source.split('/')[-1] if '/' in source else source
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫
        formatted_parts.append(
            f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source_name}, —Å—Ç—Ä. {page}]\n{chunk.page_content}"
        )
    
    return "\n\n---\n\n".join(formatted_parts)

def format_sources(documents):
    """
    –ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ —Ñ–∞–π–ª–∞–º
    –§–æ—Ä–º–∞—Ç: "üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: file1.pdf (—Å—Ç—Ä. 3, 5), file2.pdf (—Å—Ç—Ä. 1)"
    """
    if not documents:
        return None
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ —Ñ–∞–π–ª–∞–º
    sources_by_file = {}
    for doc in documents:
        source = doc.metadata.get('source', 'Unknown')
        source_name = source.split('/')[-1] if '/' in source else source
        page = doc.metadata.get('page', 'N/A')
        
        if source_name not in sources_by_file:
            sources_by_file[source_name] = []
        if page != 'N/A':
            sources_by_file[source_name].append(str(page))
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω–æ
    parts = []
    for filename, pages in sources_by_file.items():
        if pages:
            pages_str = ", ".join(sorted(set(pages), key=lambda x: int(x) if x.isdigit() else 0))
            parts.append(f"{filename} (—Å—Ç—Ä. {pages_str})")
        else:
            parts.append(filename)
    
    return "üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: " + ", ".join(parts)

def _load_prompts():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    global _conversational_answering_prompt, _retrieval_query_transform_prompt
    
    if _conversational_answering_prompt is not None:
        return _conversational_answering_prompt, _retrieval_query_transform_prompt
    
    try:
        conversation_system_text = config.load_prompt(config.CONVERSATION_SYSTEM_PROMPT_FILE)
        query_transform_text = config.load_prompt(config.QUERY_TRANSFORM_PROMPT_FILE)
        
        _conversational_answering_prompt = ChatPromptTemplate(
            [
                ("system", conversation_system_text),
                ("placeholder", "{messages}")
            ]
        )
        
        _retrieval_query_transform_prompt = ChatPromptTemplate.from_messages(
            [
                MessagesPlaceholder(variable_name="messages"),
                ("user", query_transform_text),
            ]
        )
        
        logger.info("Prompts loaded successfully")
        return _conversational_answering_prompt, _retrieval_query_transform_prompt
        
    except FileNotFoundError as e:
        logger.error(f"Prompt file not found: {e}")
        raise
    except Exception as e:
        logger.error(f"Error loading prompts: {e}", exc_info=True)
        raise

def _get_llm_query_transform():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –¥–ª—è query transformation —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _llm_query_transform
    if _llm_query_transform is None:
        _llm_query_transform = ChatOpenAI(
            model=config.MODEL_QUERY_TRANSFORM,
            temperature=0.4
        )
        logger.info(f"Query transform LLM initialized: {config.MODEL_QUERY_TRANSFORM}")
    return _llm_query_transform

def _get_llm():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π LLM —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _llm
    if _llm is None:
        _llm = ChatOpenAI(
            model=config.MODEL,
            temperature=0.9
        )
        logger.info(f"Main LLM initialized: {config.MODEL}")
    return _llm

def get_retrieval_query_transformation_chain():
    """–¶–µ–ø–æ—á–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞"""
    _, retrieval_query_transform_prompt = _load_prompts()
    return (
        retrieval_query_transform_prompt
        | _get_llm_query_transform()
        | StrOutputParser()
    )

def get_rag_chain():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è RAG-—Ü–µ–ø–æ—á–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∞—è answer –∏ documents –≤ LCEL —Å—Ç–∏–ª–µ"""
    if retriever is None:
        raise ValueError("Retriever not initialized")
    
    conversational_answering_prompt, _ = _load_prompts()
    
    # LCEL —Ü–µ–ø–æ—á–∫–∞ –≤ —Å—Ç–∏–ª–µ –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ –Ω–æ—É—Ç–±—É–∫–∞
    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º documents —á–µ—Ä–µ–∑ query transformation
    return (
        RunnablePassthrough.assign(
            documents=get_retrieval_query_transformation_chain() | retriever
        )
        # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ documents
        | RunnablePassthrough.assign(
            answer=lambda x: (conversational_answering_prompt | _get_llm() | StrOutputParser()).invoke({
                "context": format_chunks(x["documents"]),
                "messages": x["messages"]
            })
        )
        # –®–∞–≥ 3: –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ answer –∏ documents
        | (lambda x: {"answer": x["answer"], "documents": x["documents"]})
    )

async def rag_answer(messages):
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç RAG —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
    
    Args:
        messages: —Å–ø–∏—Å–æ–∫ LangChain messages (HumanMessage, AIMessage)
    
    Returns:
        dict: {"answer": str, "documents": list[Document]}
    """
    if vector_store is None or retriever is None:
        logger.error("Vector store or retriever not initialized")
        raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
    
    rag_chain = get_rag_chain()
    result = await rag_chain.ainvoke({"messages": messages})
    return result

def get_vector_store_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    if vector_store is None:
        return {"status": "not initialized", "count": 0}
    
    doc_count = len(vector_store.store) if hasattr(vector_store, 'store') else 0
    return {"status": "initialized", "count": doc_count}

