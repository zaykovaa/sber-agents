{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/langchain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_rag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain. Naive RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_langchain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Settings\n",
    "\n",
    "Вам нужно будет установить несколько пакетов и установить ваш OpenAI API ключ как переменную окружения с именем `OPENAI_API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-chroma beautifulsoup4\n",
    "%pip install -qU langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SAFE_KEYS = ('LANGCHAIN_TRACING_V2','LANGCHAIN_ENDPOINT','LANGCHAIN_PROJECT','LANGSMITH_ENDPOINT','LANGSMITH_PROJECT')\n",
    "for k in SAFE_KEYS:\n",
    "    if k in os.environ:\n",
    "        print(f'{k}: {os.environ[k]}')\n",
    "if os.getenv('LANGCHAIN_API_KEY'):\n",
    "    print('LANGCHAIN_API_KEY: <set>')\n",
    "if os.getenv('LANGSMITH_API_KEY'):\n",
    "    print('LANGSMITH_API_KEY: <set>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_vectorstore.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader\n",
    "Воспользуемся PyPDFLoader для загрузки документа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path = \"./documents/smirnoff_ai.pdf\",\n",
    "    mode = \"page\",\n",
    "    extraction_mode = \"plain\"\n",
    "     # headers = None\n",
    "    # password = None,\n",
    "    # pages_delimiter = \"\",\n",
    "    # extract_images = True,\n",
    "    # images_parser = RapidOCRBlobParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прочитаем каждую страницу документа и сохраним в список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page) \n",
    "       \n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на первую страницу документа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'SMIRNOFF_AI. Профиль с портфолио', 'source': './documents/smirnoff_ai.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "Профессиональная  разработка  ИИ-решений  для  бизнеса  \n",
      "Профессиональная  разработка  и  внедрение  систем  искусственного  интеллекта,  ИИ-ассистентов,  ИИ-агентов,  ИИ-ботов  и  других  решений  на  базе  генеративного  искусственного  интеллекта  для  частных  клиентов,  малого  и  среднего  бизнеса.  \n",
      "УСЛУГИ —  Разработка  AI- решений/агентов/ботов  для  автоматизации  бизнеса  —  Аудит  процессов  и  разработка  дорожной  карты  внедрения  AI  —  Консалтинг  по  разработке  и  внедрению  AI  —  Обучение  разработке  решений  на  базе  генеративного  ИИ  \n",
      "КЛЮЧЕВЫЕ\n",
      " \n",
      "КОМПЕТЕНЦИИ\n",
      " \n",
      "И\n",
      " \n",
      "ОПЫТ\n",
      " \n",
      "ИИ-ассистенты  Разработка  AI- ассистентов,  повышающих  эффективность  выполнения  рабочих  задач  сотрудников \n",
      "—  ИИ-ассистент  сотрудника  медиа-агентства  MediaWise  —  ИИ-ассистент  службы  поддержки  видеохостинга  RUTUBE  —  ИИ-ассистент  оператора  МФЦ  СПб  —  ИИ-ассистент  журналиста  Piter.tv  —  ИИ-ассистент  главного  агронома  \n",
      "ИИ-агенты  \n",
      "Разработка  AI- агентов,  выполняющих  функции  сотрудников,  частично  и  полностью  замещая  их \n",
      "—  Корпоративный  HR- бот  для  сотрудников  РЖД  —  ИИ-аналитик  ситуационного  центра  органов  власти  —  ИИ-менеджер  по  продажам  интернет-магазина  запчастей  \n",
      "Решения  на  базе  генеративного  ИИ  \n",
      "Разработка  AI- решений,  предоставляющих  инновационные  возможности \n",
      "—  Интеллектуальная  система  построения  карьерной  траектории  \n",
      "Комплексные  системы  ИИ  \n",
      "Разработка  и  встраивание  ИИ  в  существующие  системы  для  решения  комплексных  задач \n",
      "—  Система  интеллектуального  поиска  видео  для  видеохостинга  RUTUBE  —  Система  распознавания  рукописных  исторических  документов  РФ  —  Система  фактографического  анализа  архивных  документов\n",
      " \n",
      " \n",
      "Смирнов\n",
      " \n",
      "Сергей\n",
      ",\n",
      " \n",
      "к.т.н.\n",
      " \n",
      "https://t.me/smirno ﬀ _ai \n",
      " \n",
      " \n",
      "ЦИФРЫ\n",
      " \n",
      "10+  ЛЕТ  В  РАЗРАБОТКЕ  ИИ  20+  ЛЕТ  В  РАЗРАБОТКЕ  ПО  10+  ПРОЕКТОВ  4   ПОБЕДЫ  в  ИИ-ХАКАТОНАХ  \n",
      "СПЕЦИАЛИЗАЦИЯ\n",
      " \n",
      "  GenAI,  LLM,  VLM,  RAG,  NLP,  CV,  OCR  \n",
      "СТЕК\n",
      " \n",
      "ТЕХНОЛОГИЙ\n",
      " \n",
      "  AI:  Python,  PyTorch,  Haystack,  LangChain,  LangGraph  и  др.  Data:  MongoDB,  PostgreSQL,  Dremio,  Kafka  и  др.  DevOps: Docker,  Ansible,  Kubernetes,  Airﬂow  и  др.  Cloud: Yandex  Cloud,  Terraform  и  др.  \n",
      "ПРОЦЕССЫ/ПРАКТИКИ\n",
      " \n",
      "  Agile,  XP,  Microservices,  CI/CD,   IaC,  GitOps,  DocOps,  MLOps  \n",
      "НАГРАДЫ\n",
      " \n",
      "  ИИ-хакатоны  “Цифровой  прорыв”:  -  Международные  2024,  2023  -  Региональные  2024,  2023  ПРОФ- IT. Инновация  2023,2021  Премия  Рунета  2019  \n",
      "СЕРТИФИКАТЫ\n",
      " \n",
      "  Deep  Learning  School  (МФТИ)  Yandex  Cloud,  Big  Data,  ICAgile,  LeanKanban,  SAFe  Certiﬁed\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pages[0].metadata}\\n\")\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы разбиваем его на меньшие chunks. Воспользуемся RecursiveCharacterTextSplitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Теперь загрузим наши чанки в **векторное хранилище**. Одновременно с этим для каждого чанка мы создадим соответствующий векторный образ (эмбеддинг)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать эмбеддинг модель из OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать векторное хранилище InMemoryVectorStore (в памяти)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    all_splits,\n",
    "    embedding=OpenAIEmbeddings(model=\"openai/text-embedding-3-large\"), # text-embedding-3-large\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим retriever из нашего vectorstore для поиска по нему:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k is the number of chunks to retrieve\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "question = \"Вы проводите консультации?\"\n",
    "chunks = retriever.invoke(question)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Skia/PDF m136 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'SMIRNOFF_AI. Профиль с портфолио', 'source': './documents/smirnoff_ai.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}\n",
      "\n",
      "УСЛУГИ —  Разработка  AI- решений/агентов/ботов  для  автоматизации  бизнеса  —  Аудит  процессов  и  разработка  дорожной  карты  внедрения  AI  —  Консалтинг  по  разработке  и  внедрению  AI  —  Обучение  разработке  решений  на  базе  генеративного  ИИ  \n",
      "КЛЮЧЕВЫЕ\n",
      " \n",
      "КОМПЕТЕНЦИИ\n",
      " \n",
      "И\n",
      " \n",
      "ОПЫТ\n",
      " \n",
      "ИИ-ассистенты  Разработка  AI- ассистентов,  повышающих  эффективность  выполнения  рабочих  задач  сотрудников\n"
     ]
    }
   ],
   "source": [
    "print(f\"{chunks[0].metadata}\\n\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что вызов retriever выше возвращает некоторые части документа, которую наш чат-бот может использовать в качестве контекста при ответе на вопросы. И теперь у нас есть retriever, который может возвращать связанные данные из документа!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_prompt_augmentation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для объединения чанков в одну строку, чтобы позже использовать ее в prompt для задания контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat chunks into a single string to insert into the prompt\n",
    "def format_chunks(chunks):\n",
    "    return \"\\n\\n\".join(chunk.page_content for chunk in chunks)\n",
    "\n",
    "chunks_context = format_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the user question.\n",
    "If you don't find the answer in provided context strictly say 'Я не нашел ответа на ваш вопрос!'.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate([\n",
    "        (\"system\", SYSTEM_TEMPLATE),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать модель от OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"openai/gpt-oss-20b:free\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим LLM ответить на вопрос по найденным чанкам в контексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Да, в рамках наших услуг мы проводим консалтинг по разработке и внедрению AI‑решений.  \\nМы помогаем аудитировать процессы, разрабатывать дорожные карты и обучать команду.  \\nЕсли нужна персональная консультация, уточните детали, и мы подберём подходящий пакет.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 516, 'total_tokens': 644, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'openai/gpt-oss-20b:free', 'system_fingerprint': None, 'id': 'gen-1762461315-zDJ7EtlulmhwifdPPr7d', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--f3556f1a-4d12-473a-bc39-b6c3885b9ac9-0', usage_metadata={'input_tokens': 516, 'output_tokens': 128, 'total_tokens': 644, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke(question_answering_prompt.invoke(\n",
    "    {\n",
    "        \"context\": chunks_context,\n",
    "        \"question\": question,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит хорошо! Для сравнения попробуем без контекстных документов и сравним результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Я не нашел ответа на ваш вопрос!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 135, 'total_tokens': 247, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'openai/gpt-oss-20b:free', 'system_fingerprint': None, 'id': 'gen-1762461328-3MpNeA7VTIksEbrHohWv', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--8d1b3862-9500-45e8-bd4b-c87a4dfdaff6-0', usage_metadata={'input_tokens': 135, 'output_tokens': 112, 'total_tokens': 247, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question_answering_prompt.invoke(\n",
    "    {\n",
    "        \"context\": \"\",\n",
    "        \"question\": question,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Браво! Мы с вами создали первую RAG систему!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте теперь создадим цепочку chain, которая будет принимать вопрос от пользователя и возвращать ответ на основе найденных чанков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_chunks, \"question\": RunnablePassthrough()}\n",
    "    | question_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите, как удобно теперь использовать - одна строчка кода и мы получаем ответ на наш вопрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Да, мы проводим консультации по разработке и внедрению ИИ‑решений.  \\nНаша команда проводит аудит процессов, разрабатывает дорожную карту внедрения и предоставляет рекомендации по оптимизации.  \\nМы помогаем ускорить переход на ИИ и повысить эффективность вашего бизнеса.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит хорошо!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим шаблон промпта для учета всей истории диалога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"\\nYou are an assistant for question-answering tasks. Answer the user's questions based on the conversation history and below context retrieved for the last question. Answer 'Я не нашел ответа на ваш вопрос!' if you don't find any information in the context. Use three sentences maximum and keep the answer concise.\\n\\nContext retrieved for the last question:\\n\\nЧанки контекста\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Вы проводите консультации?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "CONVERSATION_SYSTEM_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Answer the user's questions based on the conversation history and below context retrieved for the last question. Answer 'Я не нашел ответа на ваш вопрос!' if you don't find any information in the context. Use three sentences maximum and keep the answer concise.\\n\\nContext retrieved for the last question:\\n\\n{context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "conversational_answering_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", CONVERSATION_SYSTEM_TEMPLATE),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "conversational_answering_prompt.invoke(\n",
    "    {\n",
    "        \"context\": \"Чанки контекста\",\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question)\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем цепочку ответа на последний заданный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def get_last_message_for_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'УСЛУГИ —  Разработка  AI- решений/агентов/ботов  для  автоматизации  бизнеса  —  Аудит  процессов  и  разработка  дорожной  карты  внедрения  AI  —  Консалтинг  по  разработке  и  внедрению  AI  —  Обучение  разработке  решений  на  базе  генеративного  ИИ  \\nКЛЮЧЕВЫЕ\\n \\nКОМПЕТЕНЦИИ\\n \\nИ\\n \\nОПЫТ\\n \\nИИ-ассистенты  Разработка  AI- ассистентов,  повышающих  эффективность  выполнения  рабочих  задач  сотрудников\\n\\nИИ-МЕНЕДЖЕР\\n \\nПО\\n \\nПРОДАЖАМ\\n \\nИНТЕРНЕТ-МАГАЗИНА\\n \\nЗАПЧАСТЕЙ\\n \\nБЫТОВОЙ\\n \\nТЕХНИКИ\\n  Заказчик :  Zip-KRD  —  интернет-магазин  по  продаже  запчастей  и  аксессуаров  для  бытовой  техники   Проблематика,  задача:  -  Большой  поток  клиентов  и  вопросов,  которые  менеджеры  не  успевают  \\nотрабатывать\\n \\nв\\n \\nрежиме\\n \\n24х7\\n\\n-  Мгновенные  ответы  на  вопросы  клиентов  24х7,  отзывчивость  системы  -  Повышение  конверсии  обращений  в  заказы,  повышение  лояльности  клиентов  -  Экономия  ФОТ  сотрудников,  менеджеры  не  тратят  время  на  простые  диалоги  и  \\nотрабатывают\\n \\nтолько\\n \\n“особые”\\n \\nслучаи'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_message_retriever_chain = get_last_message_for_retriever_input | retriever | format_chunks \n",
    "last_message_retriever_chain.invoke({\"messages\": [\n",
    "            HumanMessage(content=question)\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=get_last_message_for_retriever_input | retriever | format_chunks\n",
    "    )\n",
    "    | conversational_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем нашу новую диалоговую цепочку с одним сообщением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы проводите консультации?\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат диалога: Да, мы проводим консультации по разработке и внедрению AI‑решений.  \n",
      "Мы помогаем аудитировать процессы, разрабатывать дорожную карту и консультируем по всем этапам проекта.  \n",
      "Если нужна более подробная информация, просто дайте знать.\n"
     ]
    }
   ],
   "source": [
    "# Тестируем диалог с одним сообщением\n",
    "answer = rag_conversation_chain.invoke({\"messages\": [\n",
    "    HumanMessage(content=question)\n",
    "]})\n",
    "print(\"Результат диалога:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все прекрасно. Мы получили адекватный ответ на наш вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь зададим уточняющий вопрос \"А ещё какие?\" (имея ввиду, а какие еще услуги или консультации предоставляются?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"А ещё какие?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат диалога с несколькими сообщениями: Мы также предлагаем услуги по интеграции и автоматизации бизнес‑процессов, настройке DevOps и MLOps, а также кросс‑платформенное тестирование и аудит данных.  \n",
      "Кроме того, занимаемся обучением команды и разработкой пользовательских AI‑моделей с использованием PyTorch, LangChain и Haystack.  \n",
      "Если нужно уточнить детали, дайте знать.\n"
     ]
    }
   ],
   "source": [
    "# Тестируем полный диалог с несколькими сообщениями\n",
    "dialog_result = rag_conversation_chain.invoke({\"messages\": [\n",
    "    HumanMessage(content=question), \n",
    "    AIMessage(content=answer),\n",
    "    HumanMessage(content=question2), # \"А ещё какие?\"\n",
    "]})\n",
    "print(\"Результат диалога с несколькими сообщениями:\", dialog_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим проблему! Наша цепочка не смогла найти ответ на вопрос.\n",
    "И действительно сам вопрос \"А какие еще?\" не несет в себе смысла без понимания истории диалога"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы понимаем, что чат-боты взаимодействуют с пользователями в режиме беседы и поэтому должны справляться с уточняющими вопросами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на чанки, которые мы получили при ответе на вопрос `А ещё что?`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  Python,  PyTorch,  Haystack,  LangChain,  LangGraph  и  др.  Data:  MongoDB,  PostgreSQL,  Dremio,  Kafka  и  др.  DevOps: Docker,  Ansible,  Kubernetes,  Airﬂow  и  др.  Cloud: Yandex  Cloud,  Terraform  и  др.  \n",
      "ПРОЦЕССЫ/ПРАКТИКИ\n",
      " \n",
      "  Agile,  XP,  Microservices,  CI/CD,   IaC,  GitOps,  DocOps,  MLOps  \n",
      "НАГРАДЫ\n",
      " \n",
      "  ИИ-хакатоны  “Цифровой  прорыв”:  -  Международные  2024,  2023  -  Региональные  2024,  2023  ПРОФ- IT. Инновация  2023,2021  Премия  Рунета  2019  \n",
      "СЕРТИФИКАТЫ\n"
     ]
    }
   ],
   "source": [
    "result_chunks = retriever.invoke(question2)\n",
    "\n",
    "print(result_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что чанк не содержит информации о других услугах. Значит мы были правы в своем предположении о том, что retriever не нашел нужной информации на запрос \"А еще какие?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Чтобы решить эту проблему, мы можем создать для retriever правильный поисковый запрос на основе всей истории переписки.\n",
    "\n",
    "Применим технику Query Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "retrieval_query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Transform last user message to a search query in Russian language according to the whole conversation history above to further retrieve the information relevant to the conversation. Try to thorougly analyze all message to generate the most relevant query. The longer result better than short. Let it be better more abstract than specific. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_query_transform = ChatOpenAI(model=\"gpt-4o\", temperature=0.4)\n",
    "\n",
    "retrieval_query_transformation_chain = retrieval_query_transform_prompt | llm_query_transform | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы создали цепочку для переписывания (трансформции) пользовательского сообщения в поисковый запрос для retriever. Проверим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дополнительные виды консультаций и услуг, предоставляемых в области искусственного интеллекта и технологий'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_query_transformation_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question), #Какие услуги предоставляются?\n",
    "            AIMessage(\n",
    "                content=answer #Да, мы проводим консалтинг по разработке и внедрению AI.\n",
    "            ),\n",
    "            HumanMessage(content=question2), #А ещё какие?\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Супер! Мы видим осмысленный запрос вместо абстрактного \"А какие еще?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь создадим цепочку, которая будет использоваться для ответа на вопросы, учитывая историю переписки. Эта цепочка должна уметь отвечать на уточняющие вопросы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_query_transform_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context= retrieval_query_transformation_chain | retriever | format_chunks\n",
    "    )\n",
    "    | conversational_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цепочка создана. Теперь проверим ее на наших нескольких сообщениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кроме консалтинга, мы предлагаем разработку AI-решений, аудит процессов и обучение разработке решений на базе генеративного ИИ. Также можем предоставить услуги по внедрению ИИ в существующие системы.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_query_transform_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question), #Какие услуги предоставляются?\n",
    "            AIMessage(\n",
    "                content=answer #Да, мы проводим консалтинг по разработке и внедрению AI.\n",
    "            ),\n",
    "            HumanMessage(content=question2), #А ещё какие?\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все! Теперь у нас есть цепочка готовая для внедрения в наш бот!\n",
    "Ура :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практика трансформации запроса, рассмотренная нами, - это всего лишь одна из множества практик и подходов построения RAG-системы в реальной жизни для работы с реальными документами и базами знаний:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_advanced.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_notebook_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
