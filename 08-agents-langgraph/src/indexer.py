import logging
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader, JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import InMemoryVectorStore
from config import config

logger = logging.getLogger(__name__)

def load_pdf_documents(data_dir: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    pages = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        logger.warning(f"Directory {data_dir} does not exist")
        return pages
    
    pdf_files = list(data_path.glob("*.pdf"))
    logger.info(f"Found {len(pdf_files)} PDF files in {data_dir}")
    
    for pdf_file in pdf_files:
        loader = PyPDFLoader(str(pdf_file))
        pages.extend(loader.load())
        logger.info(f"Loaded {pdf_file.name}")
    
    return pages

def split_documents(pages: list) -> list:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    chunks = text_splitter.split_documents(pages)
    logger.info(f"Split into {len(chunks)} chunks")
    return chunks

def load_json_documents(json_file_path: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ Q&A –ø–∞—Ä –∏–∑ JSON, –∫–∞–∂–¥–∞—è –ø–∞—Ä–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π —á–∞–Ω–∫"""
    json_path = Path(json_file_path)
    if not json_path.exists():
        logger.warning(f"JSON file {json_file_path} does not exist")
        return []
    
    try:
        loader = JSONLoader(
            file_path=str(json_path),
            jq_schema='.[].full_text',
            text_content=False
        )
        documents = loader.load()
        logger.info(f"Loaded {len(documents)} Q&A pairs from JSON")
        return documents
    except Exception as e:
        logger.error(f"Error loading JSON: {e}")
        return []

def create_embeddings():
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings –ø–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: openai, huggingface
    """
    provider = config.EMBEDDING_PROVIDER.lower()
    
    if provider == "openai":
        logger.info(f"Creating OpenAI embeddings: {config.EMBEDDING_MODEL}")
        return OpenAIEmbeddings(model=config.EMBEDDING_MODEL)
    
    elif provider == "huggingface":
        logger.info(f"Creating HuggingFace embeddings: {config.HUGGINGFACE_EMBEDDING_MODEL} on {config.HUGGINGFACE_DEVICE}")
        logger.info("‚è≥ Loading model (this may take 1-3 minutes on first run, especially on CPU)...")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name=config.HUGGINGFACE_EMBEDDING_MODEL,
                model_kwargs={'device': config.HUGGINGFACE_DEVICE},
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("‚úÖ HuggingFace model loaded successfully")
            return embeddings
        except Exception as e:
            logger.error(f"‚ùå Failed to load HuggingFace model: {e}", exc_info=True)
            raise
    
    else:
        raise ValueError(f"Unknown embedding provider: {provider}. Use 'openai' or 'huggingface'")

def create_vector_store(chunks: list):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    logger.info(f"üì¶ Creating embeddings for {len(chunks)} chunks...")
    embeddings = create_embeddings()
    logger.info("üîÑ Generating embeddings and creating vector store (this may take time)...")
    vector_store = InMemoryVectorStore.from_documents(
        documents=chunks,
        embedding=embeddings
    )
    logger.info(f"‚úÖ Created vector store with {len(chunks)} chunks")
    return vector_store

async def reindex_all():
    """–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (PDF + JSON)
    
    Returns:
        tuple: (vector_store, chunks) –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ retriever
    """
    logger.info("Starting full reindexing...")
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        pages = load_pdf_documents(config.DATA_DIR)
        pdf_chunks = split_documents(pages) if pages else []
        logger.info(f"PDF: {len(pdf_chunks)} chunks")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ JSON Q&A –ø–∞—Ä
        json_file = Path(config.DATA_DIR) / "sberbank_help_documents.json"
        json_documents = load_json_documents(str(json_file))
        logger.info(f"JSON: {len(json_documents)} Q&A pairs")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        all_chunks = pdf_chunks + json_documents
        
        if not all_chunks:
            logger.warning("No documents found to index")
            return None, []
        
        logger.info(f"Total chunks to index: {len(all_chunks)} (PDF: {len(pdf_chunks)}, JSON: {len(json_documents)})")
        
        vector_store = create_vector_store(all_chunks)
        logger.info("Reindexing completed successfully")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º vector_store –∏ chunks –¥–ª—è BM25
        return vector_store, all_chunks
        
    except FileNotFoundError as e:
        logger.error(f"File not found: {e}")
        return None, []
    except Exception as e:
        logger.error(f"Error during reindexing: {e}", exc_info=True)
        return None, []

